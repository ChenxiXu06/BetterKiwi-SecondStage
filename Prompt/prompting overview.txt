Prompting Strategy for BetterKiwi

Part I 
## Overview
The code in this folder serves as the original mind for our DIC, which uses OpenAI's API and GPT-4-turbo model to generate complex and tricky multiple-choice questions (MCQs) related to computer science topics (we use data structures course as an example). 

It simulates the role of a difficult teacher who intentionally designs questions to test deep understanding and careful reasoning.

Part II Explanation for the initial code

## System Prompt
The assistant is initialized with a system prompt that establishes the tone and purpose of the assistant:
> "You are a very tedious teacher and computer scientist who loves giving very complicated and confusing multiple choice questions to students for a quiz."

This primes the model to:
- Generate questions with misleading answer choices.
- Emphasize depth of understanding.
- Provide a reasoning process (chain-of-thought or CoT, which will be talked in detail in "COT overview.txt") behind the question design.

## User Interaction Flow
1. The user inputs a data structure topic (e.g., "linked lists", "trees").
2. The assistant is instructed to generate a **single 'nasty and confusing' question** on the topic along with a **Chain of Thought (CoT)** explaining its rationale.
3. The assistant then continues generating two more questions through iterative messages.
4. For each question, the assistant is asked to self-evaluate its difficulty and quality with a rating from 1 to 5.

## Iterative Prompting (Initially, we assigned the generator to generate 5 tricky questions)
The dialogue history accumulates with every step:
- After each generated question, the assistant's reply is appended to the history.
- A follow-up user prompt requests a rating for that specific question.
- This method ensures context awareness and consistency across generations.

## Prompting Highlights
- The assistant is reminded of the original goal through another system prompt:
> "You should generate every question like the COT process"

This reinforces the importance of CoT reasoning for each subsequent question.
- Questions and ratings are saved in `questions.txt` and `ratings.txt` respectively, creating a traceable and reusable dataset.



