# Prompting Strategy for BetterKiwi

## Overview
Our code uses OpenAI's API and GPT-4-turbo model to generate complex and tricky multiple-choice questions (MCQs) related to computer science topics (we use data structures course ). It simulates the role of a difficult teacher who intentionally designs questions to test deep understanding and careful reasoning.

## System Prompt
The assistant is initialized with a system prompt that establishes the tone and purpose of the assistant:
> "You are a very tedious teacher and computer scientist who loves giving very complicated and confusing multiple choice questions to students for a quiz."

This primes the model to:
- Generate questions with misleading answer choices.
- Emphasize depth of understanding.
- Provide a reasoning process (chain-of-thought or CoT) behind the question design.

## User Interaction Flow
1. The user inputs a data structure topic (e.g., "linked lists", "trees").
2. The assistant is instructed to generate a **single 'nasty and confusing' question** on the topic along with a **Chain of Thought (CoT)** explaining its rationale.
3. The assistant then continues generating two more questions through iterative messages.
4. For each question, the assistant is asked to self-evaluate its difficulty and quality with a rating from 1 to 5.

## Iterative Prompting
The dialogue history accumulates with every step:
- After each generated question, the assistant's reply is appended to the history.
- A follow-up user prompt requests a rating for that specific question.
- This method ensures context awareness and consistency across generations.

## Prompting Highlights
- The assistant is reminded of the original goal through another system prompt:
> "You should generate every question like the COT process"

This reinforces the importance of CoT reasoning for each subsequent question.
- Questions and ratings are saved in `questions.txt` and `ratings.txt` respectively, creating a traceable and reusable dataset.

## Future Improvements
- Add user customization for number of questions or difficulty levels.
- Include a multiple-choice answer key with explanations.
- Implement feedback loops where users can upvote/downvote questions for training.

## Conclusion
This prompting structure balances creative generation with quality control through role priming and CoT reasoning. It's ideal for generating diverse and pedagogically rich CS quiz material.

